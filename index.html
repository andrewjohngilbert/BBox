<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Obtaining an accurate input mask specifying the location and scale of the object in a new image can be highly challenging. To overcome such limitations, we define a novel problem of \textit{unconstrained generative object compositing}, i.e., the generation is not bounded by the mask, and train a diffusion-based model on a synthesized paired dataset. Our first-of-its-kind model is able to generate object effects such as shadows and reflections that go beyond the mask, enhancing image realism.">
  <meta property="og:title" content="Thinking Outside the BBox: Unconstrained Generative Object Compositing"/>
  <meta property="og:description" content="Obtaining an accurate input mask specifying the location and scale of the object in a new image can be highly challenging. To overcome such limitations, we define a novel problem of \textit{unconstrained generative object compositing}, i.e., the generation is not bounded by the mask, and train a diffusion-based model on a synthesized paired dataset. Our first-of-its-kind model is able to generate object effects such as shadows and reflections that go beyond the mask, enhancing image realism."/>
  <meta property="og:url" content="https://andrewjohngilbert.github.io/BBox/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="assets/BBox_Teaser.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Thinking Outside the BBox: Unconstrained Generative Object Compositing">
  <meta name="twitter:description" content="Obtaining an accurate input mask specifying the location and scale of the object in a new image can be highly challenging. To overcome such limitations, we define a novel problem of \textit{unconstrained generative object compositing}, i.e., the generation is not bounded by the mask, and train a diffusion-based model on a synthesized paired dataset. Our first-of-its-kind model is able to generate object effects such as shadows and reflections that go beyond the mask, enhancing image realism.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="assets/BBox_Teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Thinking Outside the BBox: Unconstrained Generative Object Compositing</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon-32x32.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Thinking Outside the BBox: Unconstrained Generative Object Compositing </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/gemmacanettarres" target="_blank">Gemma C Tarrés</a><sup>[1]</sup>,</span>
                  <span class="author-block">
                    <a href="https://research.adobe.com/person/zhe-lin/" target="_blank">Zhe Lin</a><sup>[2]</sup>,</span>
                    <span class="author-block">
                      <a href="https://research.adobe.com/person/zhifei-zhang/" target="_blank">Zhifei Zhang</a><sup>[2]</sup>,</span>                    
                      <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=TkVHKDgAAAAJ&inst=15262737669262836719" target="_blank">Jianming Zhang</a><sup>[2]</sup>,</span>
                        <span class="author-block">
                          <a href="https://song630.github.io/yizhisong.github.io/" target="_blank">Yizhi Song</a><sup>[2]</sup>,</span>    
                          <span class="author-block">
                            <a href="https://danruta.co.uk/" target="_blank">Dan Ruta</a><sup>[1]</sup>,</span>                                                
                  <span class="author-block">
                    <a href="https://andrewjohngilbert.github.io/" target="_blank">Andrew Gilbert</a><sup>[1]</sup>,</span>
                  <span class="author-block">
                    <a href="http://personal.ee.surrey.ac.uk/Personal/J.Collomosse/" target="_blank">John Collomosse</a>[1,2]
                  </span>
                  <span class="author-block">
                    <a href="https://sites.google.com/view/sooyekim" target="_blank">Soo Ye Kim</a>[2]
                  </span>                  
                  </div>




                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Unviersity of Surrey[1], Adobe Resarch [2]<br>European Conference on Computer Vision ECCV'24</span>
                
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://andrewjohngilbert.github.io/BBox/assets/ECCV_2024_Outside_the_BBox_Camera_Ready.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <div class="column has-text-centered">
                      <div class="publication-links">
                           <!-- Arxiv PDF link -->
                        <span class="link-block">
                          <a href="https://www.youtube.com/watch?v=KI8rwCS5ab4" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fas fa-file-pdf"></i>
                          </span>
                          <span>Video</span>
                        </a>
                      </span>

                      <div class="column has-text-centered">
                        <div class="publication-links">
                             <!-- Arxiv PDF link -->
                          <span class="link-block">
                            <a href="https://andrewjohngilbert.github.io/BBox/assets/bbox_poster.pdf" target="_blank"
                            class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                              <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>Poster</span>
                          </a>
                        </span>

                    <!-- Supplementary PDF link -->
<!--                  <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>
-->  
                  <!-- Github link -->
              <!--    <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              -->

                
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/BBox_Teaser.png">
      <h2 class="subtitle has-text-centered">
        Our unconstrained object compositing model has various advantages. When using a bbox (bottom), our model achieves better background preservation (see bird in background) and more natural shadows and reflections than SotA models by allowing generation beyond the bbox. Without any bbox input (top), our model can automatically place and composite objects in diverse ways.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/KI8rwCS5ab4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Compositing an object into an image involves multiple non-trivial sub-tasks such as object placement and scaling, color/lighting harmonization, viewpoint/geometry adjustment, and shadow/reflection generation. Recent generative image compositing methods leverage diffusion models to handle multiple sub-tasks at once. However, existing models face limitations due to their reliance on masking the original object \sooye{during} training, which constrains their generation to the input mask. Furthermore, obtaining an accurate input mask specifying the location and scale of the object in a new image can be highly challenging. To overcome such limitations, we define a novel problem of \textit{unconstrained generative object compositing}, i.e., the generation is not bounded by the mask, and train a diffusion-based model on a synthesized paired dataset. Our first-of-its-kind model is able to generate object effects such as shadows and reflections that go beyond the mask, enhancing image realism. Additionally, if an empty mask is provided, our model automatically places the object in diverse natural locations and scales, accelerating the compositing workflow. Our model outperforms existing object placement and compositing models in various quality metrics and user studies.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Model Image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/BBox_pipeline.png">
      <h2 class="subtitle has-text-centered">
        Model architecture. Our model consists of: (i) an object encoder E and a content adaptor A that encode the object at different scales; (ii) a Stable Diffusion backbone comprised of an autoencoder (G, D) and a U-Net. The multiscale embeddings from (i) are averaged to condition the U-Net via cross-attention. Background image IBG and a mask Ip are concatenated to the input of (ii). Ip can be empty by setting all values to -1. The U-Net is adapted to return the predicted mask I'm as an additional output
      </h2>
    </div>
  </div>
</section>
<!-- End Model Image -->

<!-- Results Image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/BBox_baselinescomp.png">
      <h2 class="subtitle has-text-centered">
        Visual comparison to generative image compositing models, providing the same input object, background, and bbox. Our model generates realistic results with natural shadows and reflections while preserving the original background
      </h2>
    </div>
  </div>
</section>
<!-- End Results Image -->

<!-- Results 2 Image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/BBox_baselinespos2.png">
      <h2 class="subtitle has-text-centered">
        Visualization of location and scale prediction (marked in yellow) of our model and prior object placement prediction works. For visualization purpose, we display our generated image for our model and copy-paste the object into the predicted bounding box for the compared models.
      </h2>
    </div>
  </div>
</section>
<!-- End Results 2 Image -->


<!-- Paper poster -->
<!--
  <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->
-->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{Tarres:ECCV:2024,
        AUTHOR = Gemma C Tarrés, Zhe Lin, Zhifei Zhang, Jianming Zhang, Yizhi Song, Dan Ruta, Andrew Gilbert, John Collomosse, Soo Ye Kim",
        TITLE = "Thinking Outside the BBox: Unconstrained Generative Object Compositing​",
        BOOKTITLE = "European Conference on Computer Vision ECCV'24",
        YEAR = "2024",
        ​}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
